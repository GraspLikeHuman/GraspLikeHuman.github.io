<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration.">
  <meta name="keywords" content="Learning from Demonstration, Force and Tactile Sensing, Grasping, Multifingered Hands">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/fig_2a.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Authors<br>
              <span style="display:inline-block; height:0.5em"></span> 
              *To comply with the Rules for the Double-Anonymous Review, the author's information has been temporarily concealed.
            </span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Dataset Link. -->
              <!-- 修改后的空链接 -->
              <span class="link-block">
                <a href="./static/datasets/grasplikehumans_dataset.zip" 
                   download="grasping_dataset.zip"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>              
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset(Coming soon)</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">


      <!--/ Paper video. -->

  <div class="container is-max-desktop">
      <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/guo2025tro.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Tactile and kinesthetic perceptions are crucial for human dexterous manipulation, 
            enabling reliable grasping of objects via proprioceptive sensorimotor integration. 
            For robotic hands, even though acquiring such tactile and kinesthetic feedback is feasible, 
            establishing a direct mapping from this sensory feedback to motor actions remains challenging. 
            In this paper, we propose a novel glove-mediated tactile-kinematic perception-prediction framework
               for grasp skill transfer from human intuitive and natural operation to robotic execution based on 
               imitation learning, and its effectiveness is validated through generalized grasping tasks, including 
               those involving deformable objects. 
          Firstly, we integrate a data glove to capture tactile and kinesthetic data at the joint level. 
          The glove is adaptable for both human and robotic hands, allowing data collection from natural 
          human hand demonstrations across different scenarios. It ensures consistency in the raw data format, 
          enabling evaluation of grasping for both human and robotic hands. Secondly, we establish a unified 
          representation of multi-modal inputs based on graph structures with polar coordinates. We explicitly 
          integrate the morphological differences into the designed representation,
          enhancing the compatibility across different demonstrators and robotic hands. 
          Furthermore, we introduce the Tactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), 
          which leverage multidimensional subgraph convolutions and attention-based LSTM layers to extract 
          spatio-temporal features from graph inputs to predict node-based states for each hand joint. 
          These predictions are then mapped to final commands through a force-position hybrid mapping. 
          Comparative experiments and ablation studies demonstrate that our approach surpasses other methods 
          in grasp success rate, finger coordination, contact force management, and both grasp and computational 
          efficiency, achieving results most akin to human grasping. The robustness of our approach is also validated 
          through multiple randomized experimental setups, and its generalization capability is tested across diverse 
          objects and robotic hands.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. Modified upon original Nerfies website (<a
            href="https://github.com/nerfies/nerfies.github.io">source</a>).
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
